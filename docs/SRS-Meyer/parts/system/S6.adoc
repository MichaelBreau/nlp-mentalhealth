[#s6,reftext=S.6]
=== (S.6) Verification and acceptance criteria

ifdef::env-draft[]
TIP: _Specification of the conditions under which an implementation will be deemed satisfactory. Here, "verification" as shorthand for what is more explicitly called "Verification & Validation" (V&V), covering several levels of testing — module testing, integration testing, system testing, user acceptance testing — as well as other techniques such as static analysis and, when applicable, program proving._  <<BM22>>
endif::[]

- The system should meet the accuracy requirements for each component laid out in S.2. This will be determined by taking the labeled data from eRisk, splitting it into a train set and a test set, training the prediction models on the train set, and then predicting answers for the test set. These predictions will be evaluated against the known values of the test set on the accuracy measures laid out in S.2 and if they outperform the baselines described in S.2 the implementation will be deemed satisfactory.

- The system should perform in the top 50% of systems submitted to eRisk this year for each of the tasks. Given these results will be released by eRisk sometime after the capstone course wraps up the result of this criteria will not be included in the final capstone documentation.

- The predictions models used in the system should be generalizable beyond the eRisk data we are developing on. This is a difficult and vague thing to prove, that the system's models don't just perform well on the data they are developed on but that they will perform well on any instance of the problem the model is built to solve. For the purposes of the project this criteria will be considered met if a reasonable justification of generalizablity is made for each model, including an analysis of any factors that negatively impact generalizability.

- Within the context of the eRisk competition the models used by the system should be at least somewhat novel. From the perspective of this being a research project contributing to a research competition the approaches developed should be somewhat novel compared to past approaches by eRisk teams and ideally should build on top of past work. This criteria is vague, for the purposes of this project it will be considered met if an analysis of past years' entries show's the approaches taken by the system are not a direct recreation of any past approaches.

