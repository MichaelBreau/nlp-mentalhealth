\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{amsmath, amssymb}
\usepackage{amsfonts}
\usepackage{pdflscape}
\usepackage{longtable}

\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}
\setcounter{secnumdepth}{4}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{1/08/2024}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
11/03/2023 & 1.0 & Revision 0\\
1/08/2024 & 1.1 & Verbiage Changes \& adjustments to AI Guidelines \\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  UQAM & Université du Québec à Montréal\\
  RMSE & Root Mean Square Error\\
  MZOE & Mean Zero-One Error\\
  MAE & Mean Absolute Error\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

\section{General Information}

\subsection{Summary}

The CLEF eRisk competition, better known as the Early Risk Prediction on
the internet competition, is an organization that hosts a yearly event where
numerous research teams come together to explore new strategies and ap-
plications of early detection technologies, particularly in the field of health
and safety. The team at McMaster has partnered alongside the University of
Quebec in Montreal to help leverage their existing research and work done
in prior years of the eRisk competition to help design a new system entry
for this year. The team will not be in charge of the system operations por-
tion of the project, but instead will be focused on designing new strategies
and implementations for the Natural Language Processing portion of the
project. The NLP will be responsible for analyzing textual input from real
user data, and will use its provided training data and algorithms to deter-
mine the probability of the user showing signs of a selected mental health
issue. The competition normally consists of three different tasks, all of which
require different Natural Language processing techniques, and to help select
strategies, the team will be using data from the prior year's competition, which was used
to find symptoms for depression, pathological gambling and eating disorders.

\subsection{Objectives}

The primary objectives of the current process are to build confidence regard-
ing the various techniques and libraries required to efficiently analyze the
datasets provided by the eRisk Competition. The testing outlined in this
document is to help demonstrate the team's familiarity and competency to adapt to
the various challenges presented by eRisk, whether that be through sentence
ranking, early detection through post history, or estimation of severity level
through user submissions. This is meant to build confidence for the project's stake-
holders on the code efficiency, accuracy and reliability. The exact number of
challenges that the team will participate in is variable, as the scope ensures
competing in at least one challenge, as even though the project consists of a larger team than prior years, past teams have not always been able to take on
all three challenges, extending ourselves to take on all tasks would be going
beyond SRS expectations. An additional objective is to increase efficiency
and accuracy of the existing work done by the team in prior years, this is
a goal for both the team and the stakeholders at UQAM, but it is not a
formalized metric in the SRS, as there is no defined expectation of how much
better the project is supposed to preform.

\subsection{Relevant Documentation}
\begin{itemize}
	\item \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/ProblemStatementAndGoals/ProblemStatement.pdf}{Problem Statement}
\end{itemize}
The Problem statement document abstractly identifies the problem to be
solved, characterizing it in terms of its inputs and outputs, which gives con-
text to the related environment and stakeholders.

\begin{itemize}
	\item \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/DevelopmentPlan/DevelopmentPlan.pdf}{Development Plan}
\end{itemize}
The Development Plan contains the project development overview including
team roles, team meeting and communication plans, git workflow and project
schedule.

\begin{itemize}
	\item \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/SRS-Meyer/index.pdf}{SRS}
\end{itemize}
The Software Requirements Specification identifies the various functional and
non-functional requirements related to the project, while providing context
for the benefits and usage scenarios of the project.

\begin{itemize}
	\item \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/HazardAnalysis/HazardAnalysis.pdf}{Hazard Analysis}
\end{itemize}
The Hazard Analysis identifies various hazards and obstacles to the project’s
development and operation, along with mitigation strategies to help deal
with those potential hazards. It carries a focus on ensuring the project can
be delivered to the standards of the eRisk competition while maintaining
safety and privacy of user data.

\begin{itemize}
	\item \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/Design/SoftArchitecture/MG.pdf}{Module Guide}
\end{itemize}
The Module Guide decomposes the eRisk NLP project into numerous mod-
ules based on the principle of information hiding and separating data struc-
tures, in order to help increase understandability of various project compo-
nents.

\begin{itemize}
	\item \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{Module Interface Specification}
\end{itemize}
The Module Interface Specification identifies modules found in the Module
Guide and decomposes them into more primitive data types and functions,
in order to better observe the module functionality and ensure it’s behaving
as intended.

\begin{itemize}
	\item \href{https://mi.mcmaster.ca/generative-artificial-intelligence-in-teaching-and-learning/#tab-content-provisional-guidelines}{McMaster AI Guidelines}
\end{itemize}
McMaster’s Provisional Guidelines on the use of AI provides useful rulesets
that helps the team ensure the safe and responsible use of
AI practises.

\begin{itemize}
	\item \href{https://ceur-ws.org/Vol-3180/paper-74.pdf}{eRisk 2022 Paper}
\end{itemize}
The prior efforts done by the UQAM team in the eRisk competition were in-
strumental in the team’s foundation of understanding and served as a jump-
ing off point for any efforts.
\section{Plan}

This section will outline the plan for verification and validation of a number of different components within this project  (\ref{Verification and Validation Team}). This section will first give an overview of the members that are considered part of the verification and validation team and what their roles and involvement will consist of. Next there will be subsections regarding verification of the project's \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/SRS/index.pdf}{SRS} (\ref{SRS Verification Plan}) along with the design plan (\ref{Design Verification Plan}) and the verification and validation plan itself (\ref{Verification and Validation Plan Verification Plan}). This section will then outline the verification plan that has been created for the implementation of the project and model (\ref{Implementation Verification Plan}). Automated Testing and verification tools that will be used throughout this project is then the next subsection that is outlined in \ref{Automated Testing and Verification Tools} followed lastly by the validation plan for the software (\ref{Software Validation Plan}).
  
  \subsection{Verification and Validation Team} \label{Verification and Validation Team}

  
  The verification and validation team will consist of the core team members (Matthew, Jessica, Ben, Yaruo and Michael) along with the Capstone professor, the team's TA, Marie-Jean and Diego from the Montreal team, and Professor Mosser.  The core team will b responsible for creating test suites that ensure correctness in the solution along with catching possible bugs and issues that may arise. The team will be responsible for creating suitable edge cases to evaluate the correctness of the work along with general automated test suites that will be automatically deployed when new code is pulled.\\
  
  The core team will be responsible for creating all test suites, along with executing them and documenting the results. The team will also be responsible for making any changes that are required after testing the code. All core team members will have a hand in all sections of testing but different team members will have different focused responsibilities. Firstly, all core team members will be responsible for documenting the results of the automated test suites when their code enters the repository through a pull request. More specifically, Yaruo and Michaels main responsibility will be creating a set of tests including edge cases for the NLP model that will ensure that our model functions as expected for a wide variety of input data. They will be required to create test suites along with automated test suites that will be run periodically when pull requests happen. Jessica, Ben and Matthew on the other hand will have the primary responsibility for training the data vs a training data set in order to determine the accuracy of the model. They will also have the responsibility to ensure code structure in the test suites that are created along with organizing the suites while Yaruo and Michaels main role is coming up with and creating the tests.\\ 
  
  The team will meet with Professor Mosser and their TA as well periodically throughout each milestone in order to discuss the requirements for the project regarding the current milestone and to help solidify expectations. This is an opportunity for the team to ask any questions they may have as well. The team also will meet with Marie-Jean and Diego from the Montreal team periodically to help guide the team with regard to requirements of the project and the eRisk competition along with what validation means to them. They will help guide the team to find what the important things are to focus on within our project. Team 8 will also provide the team feedback for every milestone. Lastly, the actual competition itself will be the final validation step when testing the model against a new set of data and reporting how the model performs.\\

  
  \subsection{SRS Verification Plan} \label{SRS Verification Plan}
  
  All of the members of the core team will be taking part in the SRS Verification Plan along with Group 8, Professor Mosser and the TA. The team will verify the contents of the \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/SRS/index.pdf}{SRS} by comparing the requirements outlined by the eRisk competition along with our team in Montreal with the requirements stated in our SRS document. This will ensure that the team has covered everything from the SRS document and can see if there is anything new that must be added. Most of the SRS verification plan will involve inspections not only from our team, but also from the team in Montreal along with peer reviews and ad hoc feedback from group 8.\\
  
  Since our project is unique in the fact that the team is submitting to a competition with a rigid output and guideline structure, the team will also have to compare the rubric for the SRS document with what has been done in the original SRS document to ensure that all the required checkpoints have been covered. On top of this, the team will verify the SRS document by going over the feedback given to on the SRS revision 0 from TA along with the feedback received from group 8 regarding the SRS revision 0. Lastly, when verifying the SRS document, the teamwe will talk with Professor Mosser and ask him any questions that arise during the reviewing and verification process within the group. \\

  These are some of the major areas the team would like to cover in the SRS verification plan to ensure a test exists for each of these components:\\

  \begin{itemize}
  \item Is a functional overview of the system provided in the SRS?
  \item Are high-level usage scenarios included?
  \item Has the software environment and all its elements been specified?
  \item Are the limitations and exclusions accurate and all encompassing of the true limitations and exclusions of the project?
  \item Are the inputs and outputs of the system specified?
  \item Is there any unnecessary overlap in our requirements?
  \item Are there any other components outside of Text Pre-Processing, Vectorization, Prediction and Output? For those 4 sections, are there any additional functional or non-functional requirements that are missing?
  \item For every function that is outlined, are the inputs specified sufficient enough to perform the required functionality?
  \item Do any requirements conflict with each other or does each requirement avoid conflicts with other requirements?
  \item Are all detailed usage scenarios covered in the document and are completed in adequate detail or are there others we could add?
  \item Does each component and requirement have a priority?
  \item Are there any chances in the prioritization chart we should add due to changes in the code since the creation of the original SRS (revision 0)?
  \item Are the requirements clear enough that it would be possible to give to an independent team for implementation and they would be able to understand the requirements?
  \item Is every requirement testable?
  \item Is the verification and acceptance criteria specific enough? If not, what quantifiable metrics can be added to each point?
  \item Has every definition, abbreviation and acronym been defined in our Glossary?
  \item Are all imposed technical choices listed in the document?
  \item Are the risks that may be present in this project along with their mitigation analysis included in this document?
  \item Is the “Requirements Process and Reports” accurate to what is expected by each of the teams listed in the “Requirements Process and Reports” section?
  \item Does the Context and Overall Objectives section paint an accurate enough description of the project's context and objectives?\\
  \end{itemize}
  
  
  \subsection{Design Verification Plan} \label{Design Verification Plan}
  
  The plan for the design verification will be to go through the SRS document and make sure that each of the outputs and inputs that were planned for are accounted for and included in our design planning and document. The team will also look at reviews given by Group 8 on the SRS document to consider any changes they may want to add or things to consider for the design. The team will go through the same process for feedback received from the TA on the SRS report. They will also utilize the guidance of Marie-Jean and Diego in order to verify that they are on the right path with the design. This assistance can help guide the design and verify that it checks all the boxes and functionalities that are required for the competition. \\

  The following section will go over some design verification questions that will be asked regarding the functions of each of the major components in our design.\\
  
  \noindent \textbf{Text Pre-Processing Component:}
  \begin{itemize}
  \item Does the system tokenize text by dividing it into individual units of words and or sub-words?
  \item Does the system convert generated tokens into lowercase to preserve consistency?
  \item Does the system remove stop word tokens (common words that do not commonly have an effect on the meaning)?
  \item Does the system reduce tokens to their root forms (Ex: “moving” to “move”)?
  \item Is the output from this component usable by the Vectorization Component?\\
  \end{itemize}
  
  
  
  \noindent \textbf{Vectorization Component:}
  \begin{itemize}
  \item Does the system convert tokens into numerical vectors in this stage?
  \item Are these numerical vectors usable by the Prediction Component of the model?\\
  \end{itemize}
  
  \noindent \textbf{Prediction Component:}
  \begin{itemize}
  \item Is there a created training model within the design?
  \item What percentage of the given data is used for training vs testing?
  \item Is there a prediction algorithm that is implemented into the solution? What is the level of accuracy and what is acceptable levels of accuracy (this will be determined further down the road with our team in Montreal on what the team deems acceptable)?
  \item Does the system make predictions using the vectorization tokens, the training model, as well as with the created prediction algorithm?\\
  \end{itemize}
  
  
  \noindent \textbf{Output Component:}
  \begin{itemize}
  \item Does the system output the predictions to the console?
  \item Is the output produced understandable by the project team?
  \item Is the output in a format that a healthcare worker would be able to read and interpret the results?
  \end{itemize}
  
  
  \subsection{Verification and Validation Plan Verification Plan}\label{Verification and Validation Plan Verification Plan}
  
  For the Verification and Validation Plan Verification Plan the team will be making use of the reviews given for the VnV Plan from group 8 as well as from the TA. This will help the team gain some insight on areas that may be missing or need more work within the document. The team will also be going through the \href{https://github.com/MichaelBreau/nlp-mentalhealth/blob/main/docs/SRS/index.pdf}{SRS} document and ensuring they have created at least 1 test for each functional and nonfunctional requirement. It is important to ensure that they have sufficient tests for the requirements. The team would also like to do some mutation testing in the code to determine what changes will affect the tests created and in what way. It will also show that there is a possibility of missing some important tests if a mutation expected to result in a change or failure, results in no failed tests.\\ 

  Below is a checklist of the type of questions and topics the team would like to cover as part of the Verification and Validation Plan Verification Plan.\\
  
  \begin{itemize}
  
  \item Is the brief summary of the software in the summary section enough to get an understanding of the software that is being tested and its general functions?
  \item Are the objectives for the VnV plan clearly outlined and is each objective covered at some point throughout the document?
  \item Is the planning section detailed with checklists for each section and steps that will be taken for each plan?
  \item Is there at least one test created for each requirement in the SRS document?
  \item Are all functions of each component being tested? This includes the Text Pre-Processing, Vectorization, Prediction and Output Components.
  \item Are there a wide variety of tests including edge cases?
  Do the non-functional requirements contain tangible quantifiable values?
  \item Is the traceability between test cases and requirements clearly shown in section 4.3?
  \end{itemize}
  
  
  \subsection{Implementation Verification Plan} \label{Implementation Verification Plan}
  
  The large majority of the implementation for this project will revolve around achieving an acceptable level of accuracy in our system’s predictions. This document goes more in depth about how the team will do this in the implementation in section 4.1.1 but the basis of this process to ensure acceptable model accuracy levels is done through train/test splits, training models on the test sets, predicting the values of test data with these models, and then comparing these predicted values with the known expected values. All of the sections just listed are key areas of the implementation and will need to be tested in order to verify our implementation. There are 4 tests that will be run, typically in tandem with each other in one automatic test suite as outlined in 4.1.1. The largest area will be involving training and testing the data. Data will be provided by eRisk and the data will be used 3 times using different splits for each copy regarding what data is used for training vs testing. These predictions on the testing data will then be evaluated on a relevant set of metrics in order to test the program's accuracy and implementation. This set of metrics has yet to be determined due to the fact that the eRisk tasks have not been released yet for this year so the team is currently unclear what metrics would be appropriate to evaluate our model at the current time. The team will also be running a series of unit tests on the program in order to test the implementation of each part and function of the code. These tests will be recorded in the unit testing plan.\\
    
  As new code is added to our repository and as the team works on our model, we will be using implementation verification techniques like code walk-throughs and code inspections periodically. Every time a new pull request is made, a fellow teammate will be required to perform a code inspection for that pull request and give feedback. If there is something unclear, the two teammates will meet and the reviewer will receive a code walk-through from the reviewee. We will also perform code walk-throughs for new changes made to the model when we meet as a team. Lastly we plan to have continuous integration that will allow the team to run automated testing for every pull request that gets created as well as utilizing Fake9 as mentioned below to ensure our implementation follows proper coding standards.
  
  
  \subsection{Automated Testing and Verification Tools} \label{Automated Testing and Verification Tools}

  There will be three main tools the team will be using for automated testing and verification. Firstly, we will be using Flake9 which is a fork of Flake8. This will help the team follow proper coding standards and prevent problems arising regarding topics such as syntax errors, typos, incorrect styling, bad formatting and more. This will also help save time for when we do peer code reviews as a team. We will also be creating test suites using Pytest since it is a free open-sourced, simple and scalable Python-based Test Automation Framework. We will be using GitHub Actions in order to implement continuous integration and continuous delivery into our project. This will allow us to automatically build, test, and deploy our pipeline. This will create a pipeline that allows the team to build and test every pull request when they happen and also when the team deploys and merges a pull request onto another branch.
  
  \subsection{Software Validation Plan} \label{Software Validation Plan}
  
  There are a multitude of methods and parts to our software validation plan. Firstly, our primary validation method will be the actual process of competing in the eRisk competition. During the competition they will take our model and test it with new data and observe the output. This will be the final and largest validation step for us that will help validate the correctness and precision of our design. The team will also have a set of training data that will be used throughout the design process to train against, the team can use this to test how close the results are to the expected results, validating how accurate the design is. \\

  Another large aspect of the software validation plan will be our review sessions and meeting with Marie-Jean and Diego throughout the project. This will be a time for them to look at our software and run it themselves. A large checkpoint when they will have an opportunity to have our project demoed for them is around the Rev 0 demo. This will give them an opportunity to validate that our project passes all the requirements of the competition or if there is something else we need to add or change. This is an important time for us to take this feedback and use it in order to improve our project. This will also be a time for us to ask questions and for them to confirm the requirements are all being met in our design. These questions and requirements would largely surround the overall output of the design and the overall process (largely focusing on the training of the data and the prediction model created and used). It would also include validation of each of the program's components individually and as a collective (components being the Text Pre-Processing, Vectorization, Prediction and Output components). \\

  Before that however, The team will meet with professor Mosser to discuss if he thinks that all the requirements documented line up with and are accurate to the requirements outlined for the course. The team will do this with Marie-Jean and Diego as well with respect to the requirements from the competition. The team will also go over the SRS document one last time to ensure that every requirement is met. \\  
  
  Regarding the demo with Professor Mosser, Marie-Jean and Diego the team will include the following questions:
  
  \begin{itemize}
  \item Is the output format what is expected by the competition?
  \item Are there any requirements not being met within the competition (question for Marie-Jean and Diego)?
  \item Are there any requirements that are not being met regarding the project rubric from McMaster (question for Professor Mosser)?
  \item Is our text pre-processing component sufficient in which we use tokenization, lowercasing, stopword removal, and stemming or Lemmatization? If not, what should we add to our text pre-processing component or what methods that we currently have employed are not needed?
  \item Is our current method of vectorization appropriate for the task we are completing in your opinion? If not, what would you change or what different approaches do you recommend we look into.
  \item Are there any areas of our code or processes we take that are not allowed in the competition or do not follow the competitions standards?
  \item What are your thoughts on our topic modeling approach? Do you have any suggestions?
  \item Are the metrics we use for determining accuracy appropriate in your opinion? If not, what form of metrics should we be using? What level of accuracy would you consider acceptable for this metric?
  \item In your opinion is the run time of our program to output the results acceptable? If not, what would be an acceptable time?\end{itemize}
  

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

Our main requirements for the system are built around achieving an acceptable level of accuracy in our system's predictions. This is the primary focus of this section.

\subsubsection{Ensuring Model Acceptability}

The system is based in machine learning and will therefore need to be tested and evaluated using a typical machine learning evaluation method: creating train/test splits, training models on the test sets, predicting the values of test data with these models, and then comparing these predicted values with the known expected values. This section first includes information explaining how the team plans to go about this process as well as identifying areas of uncertainty before providing relevant tests.

\myparagraph{Training and Test Data} \label{TTD}

Training data given by eRisk will be split 80/20 into train and test sets. Each task has some concept of higher or lower risk, either in the form of positive/negatives, survey answers, a continuous scale, or something else. Depending on the task a metric will be created from expected values representing this risk (an example may be averaging survey answers) it is unclear what these metrics will involve currently as eRisk has not yet released this year’s tasks. Train and test splits will be created to have similar distributions of this risk metric (ie. similar numbers of low/mid/high risk individuals). How similar the distributions should be is as of yet unknown and will require further experimentation when eRisk releases their datasets for this year.

When testing the system three different train/test splits will be produced, all splits will be formed on the full training data provided by eRisk, thus giving three copies of said training data with the train and test subsets being different for each copy. These copies will try to maximize the difference between their splits. The model will be trained on a training split and then will give predictions on the test set. These predictions will be evaluated on a relevant set of metrics (outlined below in section \ref{metrics}), and an average of the metrics across all three splits will be used to determine model performance.

\myparagraph{Metrics} \label{metrics}

As the eRisk tasks have not yet been released it is unclear exactly what form the expected and predicted output will take, so it is unclear what metrics will be used to evaluate our models. If a task is a repeat from past years, metrics similar to the ones used in previous versions of the task may be used. Here are some metrics that have been deemed potentially useful to employ, whether they will be used depends on the nature of this year's eRisk tasks. Other measures will likely be added upon the release of the eRisk tasks:

\begin{enumerate}

\item{MAE} (Mean Absolute Error) was used by the UQAM team in a 2022 task about eating disorders \citep{Saravani2022MeasuringTS}. MAE measures the average distance between a predicted value $p_i$ in predictions $P$ and a ground truth (expected value) $t_i$ in ground truths $T$ for $n$ data points:

$$\text{MAE} = \sum_{i=1}^{n} \frac{|t_i - p_i|}{n}$$

\item{MZOE} (Mean Zero-One Error) was also used in the UQAM's 2022 eating disorder submission \citep{Saravani2022MeasuringTS}. MZOE measures the proportion of incorrect predictions:

$$\text{MZOE} = \frac{|\{p_i \in P : t_i \neq p_i\}|}{n}$$

\item{F-Score} was used in the UQAM team's 2021 submission for the detection of problem gambling task \citep{Maupom2021EarlyDO}. F-Score is a combination of two other measures, precision and recall, and is used to measure the accuracy of positive/negative tests while taking into account false positive and negatives:

$$\text{precision} = \frac{\text{true positives}}{\text{true positives} + \text{false positives}}$$
$$\text{recall} = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}$$
$$\text{F-score} = 2\frac{\text{precision} * \text{recall}}{\text{precision} + \text{recall}}$$

\item{RMSE} (Root Mean Square Error) is a common measure of the deviation between predicted values and expected values, it somewhat analogous to MAE:

$$\text{RMSE} = \sqrt{\sum_{i=1}^{n} \frac{(t_i - p_i)^2}{n}}$$

\item{$R^2$} measures how well the expected values are predicted by the model by comparing how much variation the predicted values capture against the variation in the expected values:

$$SS_{res} = \sum_{i=1}^{n} (t_i - p_i)^2$$
$$SS_{tot} = \sum_{i=1}^{n} (t_i - \text{mean}(T))^2$$
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

\end{enumerate}

\myparagraph{Acceptable Error Metric Values} \label{bounds}

The bounds for which error measures are considered acceptable will be based on the results other teams achieved in past eRisk competitions if the given task is a repeat. Our model should perform better than previous years. If the task is new, bounds will be determined through research into the accuracy of clinicians in these areas and the involvement of expert knowledge in the form of psychology faculty from the Université du Québec à Montréal. Our model should represent an improvement on clinicians’ diagnostic capabilities.

\myparagraph{Tests}

In practice these four tests will rarely be run separately but as one automatic test suite as the output of each test is fed to the input of the next.

\begin{enumerate}

\item{FRT-ACC1-1\\}

Control: Automatic

Initial State: No models have been trained by the system yet, the eRisk training data is available to the system.

Input: The eRisk training data.

Output: Three train/test splits where each train/test duo is composed of disjoint train and test sets whose union is all of the data in the original eRisk training data.

Test Case Derivation: The system should properly split the data into train and test sets.

How test will be performed: The eRisk training data will be fed into the system, the system will create the splits. For each split a script will check that no element exists in both the train and test split and that all training data is accounted for between both sets.

\item{FRT-ACC1-2\\}

Control: Automatic

Initial State: No models have been trained by the system yet, three train/test splits have been created.

Input: The three sets of training data.

Output: The parameters and/or objects (ie. class instances) that represent trained models.

Test Case Derivation: The system should train models on the train splits.

How test will be performed: The three training sets will be fed into the system, the system will train a model on each set. A script will check that the models exist and can produce predictions for data in the training sets.

\item{FRT-ACC1-3\\}

control: Automatic

Initial State: Three versions of the model have been trained on the relevant train sets.

Input: The three corresponding test sets into the corresponding models.

Output: Three sets of predictions.

Test Case Derivation: The system should produce some sort of prediction.

How test will be performed: The test data will be fed into the system and the system will use its trained models to produce a set of predictions. A script will check that there is a prediction for each test data point.

\item{FRT-ACC1-4\\}

Control: Automatic

Initial State: Three sets of predictions have been produced by the system.

Input: The three sets of predictions and corresponding expected values.

Output: A set of metrics formed by averaging together the metrics for each of the three sets, the specific metrics used will depend on the nature of the specific eRisk task (see section \ref{metrics}).

Acceptability of Output: For the system to be acceptable to stakeholders these metrics must be better than some baseline value determined from either past eRisk submissions for similar tasks or through research into typical clinical results (see section \ref{bounds}).

Test Case Derivation: The system must reach specific accuracy measures to be considered acceptable to stakeholders.

How test will be performed: Predictions and expected values will be given to system, which will calculate the relevant error metrics and how much they outperform baseline error metrics.

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}
\subsubsection{Usability Requirements}
		

\begin{enumerate}

\item{NFRT-U-1\\}

Type: Dynamic, Manual
					
Initial State: System is completed and trained
					
Input/Condition: The system will be ran on a Windows desktop/laptop and macOS desktop/laptop device with the correct environment setup
					
Output/Result: The system should generate an expected result 
					
How test will be performed: After setting up the environment on to both macOS and Windows machines, the system will be ran on both environment with the same command. Test will pass as long as an expected result is generated from both machines.


\end{enumerate}


\subsubsection{Safety and Security Requirements}


\begin{enumerate}

\item{NFRT-SS-1\\}

Type: Dynamic, Manual
					
Initial State: The system is completed and trained 
					
Input/Condition: The system will be ran on the developer's computer
					
Output/Result: The generated result should not overuse the processing power of the developer's computer
					
How test will be performed: CPU, GPU, RAM and live power usage will be monitored while running the software. Usage of all aspects should not increase more than 20 percent.   
					
\end{enumerate}

\begin{enumerate}

	\item{NFRT-SS-2\\}
	
	Type: Automatic, Dynamic
						
	Initial State: The system has been trained and is awaiting data to form predictions on.
						
	Input/Condition: A set of data that the system can predict on.
						
	Output/Result: The resulting predictions, in a form where no sensitive data from the input is present in the output.
	
	Test Case Derivation: Sensitive in this case refers to the post history of the subjects eRisk provides as training and test data. If these posts can be reconstructed from any part of our system outputs it is possible the identity of the individual could be discovered. This represents an unacceptable breach of privacy and can not happen.
						
	How test will be performed: After the system has produced it's predictions a script will be run that takes all posts in the input data, forms a string out of all consecutive three word triplets (ie. "hello my good friend" forms "hello my good" and "my good friend"), both with and without processing (removal of stopwords, punctuation, etc.), and scans the output to ensure that none of these triples are present.
	
\end{enumerate}

\subsubsection{Legal Requirements}

\begin{enumerate}

\item{NFRT-L-1\\}

Type: Static, Manual
					
Initial State: The source code and documentation is prepared
					
Input/Condition: The user reviews the entirety of the source code and related documentation
					
Output/Result: Copyright licenses, appropriate credits and/or MIT license must attached
					
How test will be performed: The testers will review the entirely of the project and check to see if appropriate copyright licenses and/or credits are included for resources that require them

\end{enumerate}


\begin{landscape}

		\subsection{Traceability Between Test Cases and Requirements}

		Requirement traceability from S2 of SRS to testing in this document.

		\begin{longtable}{|l|cccccccccccccccc|}
			\caption{Traceability Between Functional Test Cases and Functional Requirements, FR-1 to FR-10}                                                                                                                                                                                                                           \\
			\hline
			\textbf{Test IDs}   & \multicolumn{11}{c|}{\textbf{Functional Requirement IDs}}                                                                                                                                                                                                                                         \\
			\hline
			~                   & \textbf{FR1}                                              & \textbf{FR2} & \textbf{FR3} & \textbf{FR4} & \textbf{FR5} & \textbf{FR6} & \textbf{FR7} & \textbf{FR8} & \textbf{FR9} & \textbf{FR10}  \\
			\hline
			\textbf{FRT-ACC1-1}  & ~                                                         & ~            & ~            & ~            & ~            & X            & ~            & ~            & ~            & ~             \\
			\textbf{FRT-ACC1-2}  & ~                                                         & ~            & ~            & ~            & ~            & ~            & X            & ~            & ~            & ~             \\
			\textbf{FRT-ACC1-3}  & X                                                         & X            & X            & X            & X            & ~            & ~            & X            & ~            & ~             \\
			\textbf{FRT-ACC1-4}  & ~                                                         & ~            & ~            & ~            & ~            & ~            & ~            & ~            & X            & X             \\
			\hline
		\end{longtable}

		\begin{longtable}{|l|cccccccccccccccc|}
			\caption{Traceability Between Non-Functional Test Cases and Non-Functional Requirements}                                                                                                                                                                                                                           \\
			\hline
			\textbf{Test IDs}   & \multicolumn{11}{c|}{\textbf{Non-Functional Requirement IDs}}                                                                                                                                                                                                                                         \\
			\hline
			~                   & \textbf{GR1}                                              & \textbf{GR2} & \textbf{SR1} & \textbf{SR2} \\
			\hline
			\textbf{NFRT-U-1}  & X                                                         & ~            & ~            & ~             \\
			\textbf{NFRT-SS-1}  & ~                                                         & ~            & X            & ~            \\
			\textbf{NFRT-SS-2}  & ~                                                         & ~            & ~            & X            \\
			\textbf{NFRT-L-1}  & ~                                                         & X            & ~            & ~             \\
			\hline
		\end{longtable}
\end{landscape}

\newpage


\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

Type: Automatic, Dynamic
					
Initial State: The system has been trained and is awaiting data to form predictions on.
					
Input/Condition: A set of data that the system can predict on.
					
Output/Result: The resulting predictions, in a form where no sensitive data from the input is present in the output.

Test Case Derivation: Sensitive in this case refers to the post history of the subjects eRisk provides as training and test data. If these posts can be reconstructed from any part of our system outputs it is possible the identity of the individual could be discovered. This represents an unacceptable breach of privacy and can not happen.
					
How test will be performed: After the system has produced it's predictions a script will be run that takes all posts in the input data, forms a string out of all consecutive three word triplets (ie. "hello my good friend" forms "hello my good" and "my good friend"), both with and without processing (removal of stopwords, punctuation, etc.), and scans the output to ensure that none of these triples are present.

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

\subsection{Symbolic Parameters}

None.

\subsection{Reflection}

\begin{enumerate}
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage etc.  You should look to
  identify at least one item for each team member.

The following knowledge and skills will need to be acquired by the team to sucesfully
complete the verification and validation for the project.

1. Github Actions

2. Pytest

3. Mental Health Diagonostic Basics (For creating expected values)

4. Python script for parsing and checking files

5. System performance diagonistic skills

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?

1. Michael will acquire the skill/knowledge required for this skill due to being
the owner of the repository. Approaches for this skill/knowledge can be to either
research the topic on the github actions webpage or watch YouTube tutorials to
find the required skillset.

2. Jessica will acquire the skill/knowledge required for this skill due to being
interested in the skill. Approaches for this skill/knowledge can be to either
research the topic on the pytest documention webpage or watch YouTube tutorials to
find the required skillset.

3. Ben will acquire the skill/knowledge required for this skill due to being
interested in the skill. Approaches for this skill/knowledge can be to either
research the topic with various papers or watch YouTube lectures to
learn more about the required skillset.

4. Matthew will acquire the skill/knowledge required for this skill due to being
interested in the skill. Approaches for this skill/knowledge can be to either
research the skill using the documentation of
some python libraries for parsing documents or watch YouTube tutorials to
find the required skillset.

5. Yaruo will acquire the skill/knowledge required for this skill due to being
interested in the skill. Approaches for this skill/knowledge can be to either
research the topic on various papers or watch YouTube tutorials to
find the required skillset.
\end{enumerate}

\end{document}